---
title: " IE 48B Time Series Analytics: HW2 "

author: " Seyma Cakir - 2017402024 "
date:  " 2021-11-22 " 
output:
 html_document:
   toc: true
   toc_float: true
   smooth_scroll : true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, echo = TRUE, warning = FALSE, message = FALSE)
```



```{r}
library(data.table)
library(ggplot2)
library(tidyr)
require(TSrepr)
require(dplyr)
require(rpart)
library(knitr)
require(caret)
library(genlasso)
library(tidyverse)

setwd('D:\\Users\\seyma\\Documents\\GitHub\\fall21-seymacakir\\HW2')
```


```{r}
set.seed(488)
```




# Penalized Regression Approaches for Time Series Representation

In this study, the CBF data from [Time Series Classification Web Site](https://www.timeseriesclassification.com/dataset.php) is used for representing CBF training series using 1D Fused Lasso and regression trees. The aim is comparing  two alternative adaptive piece wise constant approximation in terms of their representation ability and classification performance.

First of all, the general descriptive of data is examined. 

```{r}
#test = read.table("CBF_TEST.txt")
train_data= as.data.table( read.table("CBF_TRAIN.txt"))
kable(head(train_data), caption = "Example of Data")
```

As seen above, the first column is reperesent class of the times series, and the other variables is the values during the time for each time series. 
Therefore, the data is melt as the first column represent ID of time series, second column represents class indicator, third column represents the time, finally last column shows the time series observation of the time series with each has x id at y time.  
```{r}
setnames(train_data,'V1','class')
train_data=train_data[order(class)]
train_data$class = as.factor(train_data$class)
train_data[,id:=1:.N]
# melt the data for long format
long_train=melt(train_data,id.vars=c('id','class'))
#head(long_train)

# need to get numerical part of the variable (represents time)
# using gsub to set the nonnumerical part to zero length
long_train[,time:=as.numeric(gsub("\\D", "", variable))-1]

# remove variable
long_train=long_train[,list(id,class,time,value)]
long_train=long_train[order(id,time)]

# check
head(long_train)



```



```{r}
ggplot() + geom_line(aes(x = long_train[id == 1]$time, y = long_train[id ==1 ]$value, color = "class 1")) +
  geom_line(aes(x = long_train[id == 12]$time, y = long_train[id ==12 ]$value, color = "class 2")) +
  geom_line(aes(x = long_train[id == 30]$time, y = long_train[id == 30 ]$value, color = "class 3")) +
  labs( 
    title = "Time Series Examples from each Class",
    x = 'time',
    y = "value") + 
  theme_minimal()
  
```


## Subtask 1: Find the time series reperesantation with 1D Fused Lasso 

In this part of the study, the each time series is represented by using 1D Fused Lasso approach.The Cross-Validation( k= 10) approach is used for the determine suitable lambda value for each series. 

The [genlosso](https://cran.r-project.org/web/packages/genlasso/vignettes/article.pdf) package is used for this purpose.

At first, the first of  time series is used to show detailed work. 

```{r}
df1 = as.numeric(long_train[id==1]$value)
out1 = trendfilter(df1, ord=0)
cv1 = cv.trendfilter(out1)
plot(out1, lambda=cv1$lambda.min, main="Minimal CV error")
```


Then, the function is generated  to get lambda variable minimizes the cross-validation error for a time series, dnd turns the lambda value and predictions for the model generated by using this lambda value. 

```{r}

get_lambda <- function (data){
  out = trendfilter(data, ord=0)
  cv = cv.trendfilter(out)

  multi_return <- list("lambda" = cv$lambda.min, "predictions" = predict(out, lambda= cv$lambda.min)$fit)
}

```

The function is applied for all time series, the lambda parameter calculated for all series and recorded. 

```{r}
lambda_values= data.table()
lambda_values[,id := 1:30]
lambda_values[, lambda:=0]

for ( i in 1:30){
  data = as.numeric(long_train[id == i ]$value)
  lambda_values[id == i]$lambda = get_lambda(data)$lambda
}

head(lambda_values)

```

the lambda values are plotted to see if it differs in class of time series.

```{r}
 lambda_values[,class:= 0]
for (i in 1:30) {
lambda_values$class[i] = long_train[id ==i]$class[i]
}
lambda_values = as.data.table(lambda_values)


ggplot(lambda_values[order(lambda)], aes(x = 1:30, y = lambda, color = factor( class))) + geom_point() +
  labs(
  title = "Lambda Values"
  ) + 
  theme_minimal() +
  theme(axis.line=element_blank(),axis.text.x=element_blank(),
          axis.title.x=element_blank())


```

However, the grapgh shows that lambda values are not distinct by class. 

## Subtask 2: Find the time series reperesantation with Regression Tree

In this task, the time series are represented with rpart regression tree. The parameters of trees are determined as complexity parameter to zero, minsplit to 20 and minbucket to 10 and the suitable max depth parameter is searched by cross-validation approach. 
It is used 1,2,3,4,5,6,7,8,9, and 10 as maxdepth parameters. 




```{r}

find_max_depth <- function(folds,df){

results <- matrix(0,nrow = 10, ncol= 10)
results <- as.data.table(results)
setnames(results, colnames(results), as.character(c(1:10)))

for (j in 1:10){
# in this loop different max_depth values are trained.
for (i in 1:10) {
# for this loop the max depth value is trained for 10 different data set and evaluated. 
  train <- df[-folds[[i]]]
  test <- df[folds[[i]]]
  
  model <- rpart(value~time, data= train, method= "anova",control= rpart.control(minsplit = 20, minbucket = 10, cp= 0, maxdepth = j))
  results[i,j] <-(mse(predict(model,test), test$value))

}
}
results[,k:= 1:10]

multi_return <- list("average_mse" = results[,lapply(.SD,mean)],"max_depth" = which.min(results[,lapply(.SD,mean)]),"matrix" = results )

return(multi_return)

}



```


The first time series is examined below and the other time series also calculated by using same approach.

the data divided into 10 different folds and in find_max_depth function above the every fold is excluded as test set and the remaining folds are used for test. 
For each max depth value the process is repeated. 
At the end, the model is works as 10 times for 1 to 10 max depth option and MSE values are calculated to determine suitable max depth parameter. 

```{r}
df = long_train[id == 1 ]
folds <- createFolds(df$value, k=10)
kable(find_max_depth(folds,df)$matrix[,c(11,1:10)], caption = "the MSE values for each max depth paramater for each fold. ")


```
By taking the avarage of MSE values for each max depth value, if the max depth paramater has the minumum average MSE value it is selected. 


**Average closs-validation MSE values for the first timer series**

```{r}
find_max_depth(folds,df)$average_mse[,1:10]
```

the minumum mse value is obtained by max depth parameter equals 3. 


the predicted values vs. real variable with parameter maxdepth = 3 for time series 1 is plotted to see how the rpart represtantation is works. 


```{r}
model2 <- rpart(value~time, data= df, method= "anova",control= rpart.control(minsplit = 20, minbucket = 10, cp= 0, maxdepth =2 ))
model3 <- rpart(value~time, data= df, method= "anova",control= rpart.control(minsplit = 20, minbucket = 10, cp= 0, maxdepth =3 ))
model4 <- rpart(value~time, data= df, method= "anova",control= rpart.control(minsplit = 20, minbucket = 10, cp= 0, maxdepth =4 ))


ggplot(df, aes( x = time, y = value), color = "Real_Values" )+ geom_point() + geom_line(aes( x = df$time, y = predict(model2), color = "max_depth = 2")) +
  geom_line(aes( x = df$time, y = predict(model3), color = "max_depth = 3")) +
  geom_line(aes( x = df$time, y = predict(model4), color = "max_depth = 4 ")) +
  labs(
    title = " Time series 1 represtantation with rpart"
    ) + theme_minimal()
```

And the process is repated for all time series. 


```{r}

max_depth <- data.table("id" = 1:30)
max_depth[, max_depth:= 30]

for (i in 1:30){
  
  df = long_train[id == i ]
  folds <- createFolds(df$value, k=10)
  
  max_depth[id==i]$max_depth <- find_max_depth(folds,df)$max_depth
  
}

max_depth[, class:= as.factor(lambda_values$class)]

head(max_depth)

```

## Subtask 3: Compare two represetation by MSE


To calculate Fused Lasso MSEs values, the get_lambda function is created in subtask 1 which allows lambda values minimizes the cross-validation error and the predictions were made for each time series. 

 
```{r}
mse_fusedlasso <- as.data.table(c(1:30))
mse_fusedlasso[,mse:= 'NA']
mse_fusedlasso[,class:= 'NA']
fusedlasso_predictions <- list()

for ( i in 1:30){
mse_fusedlasso[V1 == i]$mse <- mse(long_train[id == i]$value, get_lambda(long_train[id==i]$value)$prediction)
fusedlasso_predictions[[i]] <- get_lambda(long_train[id==i]$value)$prediction
mse_fusedlasso$class[i] <- long_train[id==i]$class[1]
}
```


**MSE values by Fussed Lasso**

```{r}

mse_fusedlasso
```

To calculate MSE values of rpart representation, in find_max_depth function the max_depth paramaters are selected for each series by using cross validation. 
By set the max depth parameter, the predictions are made and MSE values are calculated. 

```{r}
mse_rpart <- as.data.table(c(1:30))
mse_rpart[,mse:= 'NA']
rpart_predictions <-list()
mse_rpart[,class:= 'NA']

for (i in 1:30){
  
model <- rpart(value~time, data= long_train[id == i] , method= "anova",control= rpart.control(minsplit = 20, minbucket = 10, cp= 0, maxdepth =  max_depth[id==1]$max_depth))
rpart_predictions[[i]] <- predict(model)
mse_rpart[V1 == i]$mse <- mse(long_train[id == i]$value, predict(model))
mse_rpart$class[i] <- as.factor(long_train[id==i]$class[1])

}
```

**MSE values by rpart representation**
```{r}

mse_rpart

```

The mse values comparing with box-plot below to see model performances. 

```{r}

mse_values <- as.data.table(as.numeric(c(mse_fusedlasso$mse,mse_rpart$mse)))

mse_values[,model:= rep(c("fussedlusso","rpart"), each = 30)]

 ggplot(mse_values, aes(x = model, y = V1)) + geom_boxplot() + labs( title = "MSE values in Models", xlab = "model", ylab = "mse values") + theme_minimal()
```


By observing box-plot above, it can be said that fussed lusso model mse values is lower than rpart model in general, if the goal was predict time series may be fussed lasso model can be more successful. 


```{r}
mse_values <- as.data.table(as.numeric(c(mse_fusedlasso$mse,mse_rpart$mse)))

mse_values[,model:= rep(c("fusedlasso","rpart"), each = 30)]
mse_values[, class:= as.factor(c(mse_fusedlasso$class,mse_rpart$class))]

 ggplot(mse_values, aes(x = model, y = V1)) + geom_boxplot() + labs( title = "MSE values in Models differs in class", xlab = "model", ylab = "mse values") + theme_minimal() + facet_wrap(~class)
```


As seen above, fused lasso is better for all classes. 


## Subtask 4: Comparison based on the Accuracy of 1-NN Classifier

In this part of the study, the class of time series are predicted by using euclidian distance. 


At first, the time series are predicted by calculating each point distabce from another time series and the closest time series assumed that has the same time series class. 

```{r}
trainclass= train_data[,1]
traindata=train_data[,2:ncol(train_data)]
dim(traindata)
```

The code chunk below shows the 1-NN classification approach by using euclidean distance to predict first time series classification. 

```{r}
euc_dist=dist(traindata)
euc_dist=as.matrix(euc_dist)
large_number=100000
diag(euc_dist)=large_number
neighborhood=order(euc_dist[1,])
neighborhood
neighbor=neighborhood[1] 
prediction=trainclass[neighbor]

prediction


```


```{r}
neighborhood=apply(euc_dist,1,order)
predicted= trainclass[neighborhood[1,]]
```


**Confusion Matrix for rawdata**

```{r}
confusionMatrix(data=predicted$class, reference = trainclass$class)
 
```

**Classification by Rpart**

```{r}
rparttraindata <- t(as.data.table(rpart_predictions))
euc_distRPART=dist(rparttraindata)
euc_distRPART=as.matrix(euc_distRPART)
large_number=100000
diag(euc_distRPART)=large_number

neighborhoodRPART=apply(euc_distRPART,1,order)
predictedrpart= trainclass[neighborhoodRPART[1,]]
```


**Confusion Matrix for rpart time series representation**

```{r}

confusionMatrix(data=predictedrpart$class, reference = trainclass$class)

```

**Classification by FusedLasso**

```{r}
fused_lasso_traindata <- t(as.data.table(fusedlasso_predictions))
euc_distFL=dist(fused_lasso_traindata)
euc_distFL=as.matrix(euc_distFL)
large_number=100000
diag(euc_distFL)=large_number

neighborhoodFL=apply(euc_distFL,1,order)
predictedFL= trainclass[neighborhoodFL[1,]]
```

**Confusion Matrix for Fused Lasso time series representation**

```{r}

confusionMatrix(data=predictedFL$class, reference = trainclass$class)


```

By evaluating accuracy of all class predictions, it can be say that by using euclidean distance approach with 1-NN classification give better result by no time series representation.


## References 

[Genlasso Package](https://cran.r-project.org/web/packages/genlasso/vignettes/article.pdf)
[TS classification Lecture Notes By Mustafa BaydoÄŸan](https://moodle.boun.edu.tr/mod/resource/view.php?id=505544)



### RMD 

The code of my study is available from [here](https://bu-ie-48B.github.io/fall21-seymacakir/HW2/HW2.Rmd)

